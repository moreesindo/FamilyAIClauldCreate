# FamilyAI Environment Configuration Template
# Copy this file to .env and customize for your deployment

# =============================================================================
# Jetson Thor Configuration
# =============================================================================
JETSON_THOR_IP=192.168.1.100
JETSON_THOR_HOSTNAME=familyai-thor

# =============================================================================
# Proxy Configuration
# =============================================================================
# HTTP/HTTPS proxy for model downloads and container networking
# IMPORTANT: Use host machine IP, NOT 127.0.0.1 (which points to container itself)
#
# Method 1: Use host machine IP (Recommended for Jetson Thor)
PROXY_URL=http://192.168.3.84:2526
#
# Method 2: Use Docker bridge gateway (if proxy runs on host)
# PROXY_URL=http://172.28.0.1:2526
#
# Method 3: If proxy runs inside Docker network
# PROXY_URL=http://proxy-container:2526

# No proxy exceptions (don't proxy these addresses)
NO_PROXY=localhost,127.0.0.1,172.28.0.0/16,192.168.3.84

# =============================================================================
# HuggingFace Configuration
# =============================================================================
# HuggingFace token for model downloads (optional, only needed for gated models)
# HF_TOKEN=your_huggingface_token_here

# Model cache directory
HF_HOME=/home/${USER}/.cache/huggingface
TRANSFORMERS_CACHE=${HF_HOME}/hub

# =============================================================================
# vLLM Configuration
# =============================================================================
# vLLM Docker image (NVIDIA Triton Server with vLLM)
VLLM_IMAGE=nvcr.io/nvidia/tritonserver:25.08-vllm-python-py3

# GPU memory utilization (0.0 to 1.0)
VLLM_GPU_MEMORY_UTILIZATION=0.9

# Quantization method (awq, gptq, or none)
VLLM_QUANTIZATION=awq

# Enable CUDA graphs for better performance
VLLM_ENABLE_CUDA_GRAPH=true

# Tensor parallel size (number of GPUs to use per model)
VLLM_TENSOR_PARALLEL_SIZE=1

# =============================================================================
# Code Assistant Services
# =============================================================================
# Traditional code tasks
CODE_TRADITIONAL_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct
CODE_TRADITIONAL_PORT=8001
CODE_TRADITIONAL_MAX_MODEL_LEN=32768

# Agentic workflows
CODE_AGENTIC_MODEL=Qwen/Qwen3-Coder-30B-A3B-Instruct
CODE_AGENTIC_PORT=8002
CODE_AGENTIC_MAX_MODEL_LEN=131072

# =============================================================================
# Chat Services
# =============================================================================
# Advanced reasoning
CHAT_ADVANCED_MODEL=Qwen/Qwen3-32B-Instruct
CHAT_ADVANCED_PORT=8003
CHAT_ADVANCED_MAX_MODEL_LEN=32768

# Fast response
CHAT_FAST_MODEL=Qwen/Qwen3-8B-Instruct
CHAT_FAST_PORT=8004
CHAT_FAST_MAX_MODEL_LEN=32768

# Lightweight interaction
CHAT_LIGHT_MODEL=Qwen/Qwen3-4B-Instruct
CHAT_LIGHT_PORT=8005
CHAT_LIGHT_MAX_MODEL_LEN=32768

# =============================================================================
# Vision Service
# =============================================================================
VISION_MODEL=Qwen/Qwen2-VL-7B-Instruct
VISION_PORT=8006
VISION_MAX_MODEL_LEN=32768

# =============================================================================
# Speech Services
# =============================================================================
# Whisper ASR
WHISPER_MODEL=openai/whisper-small
WHISPER_PORT=8007
WHISPER_LANGUAGE=auto  # or specify: en, zh, etc.

# Piper TTS
PIPER_MODEL=en_US-lessac-medium
PIPER_PORT=8008
PIPER_VOICE=lessac  # Voice name

# =============================================================================
# Gateway Configuration
# =============================================================================
GATEWAY_PORT=8080
GATEWAY_LOG_LEVEL=INFO

# Routing thresholds
GATEWAY_CODE_CONTEXT_THRESHOLD=8192  # tokens, switch to agentic model if exceeded
GATEWAY_CHAT_SIMPLE_MAX_TOKENS=100   # route to lightweight model
GATEWAY_CHAT_COMPLEX_MIN_TOKENS=500  # route to advanced model

# =============================================================================
# Open WebUI Configuration
# =============================================================================
WEBUI_PORT=3000
WEBUI_SECRET_KEY=change_this_to_a_random_secret_key
WEBUI_DATA_DIR=/data/open-webui

# Authentication
WEBUI_ENABLE_SIGNUP=true
WEBUI_DEFAULT_USER_ROLE=user

# Integration
WEBUI_OLLAMA_BASE_URL=http://gateway:8080/v1

# =============================================================================
# Monitoring Configuration
# =============================================================================
PROMETHEUS_PORT=9090
GRAFANA_PORT=3001
GRAFANA_ADMIN_PASSWORD=change_this_password

# Alert configuration
ALERT_EMAIL_ENABLED=false
ALERT_EMAIL_TO=admin@example.com
ALERT_EMAIL_FROM=familyai@example.com
ALERT_SMTP_SERVER=smtp.gmail.com:587

# =============================================================================
# Resource Limits
# =============================================================================
# Maximum concurrent requests per service
MAX_CONCURRENT_REQUESTS=10

# Request timeout (seconds)
REQUEST_TIMEOUT=300

# Max batch size for vLLM
VLLM_MAX_BATCH_SIZE=32

# =============================================================================
# Security Configuration
# =============================================================================
# Enable authentication for API
API_AUTH_ENABLED=true
API_KEY=change_this_to_a_secure_api_key

# Enable HTTPS (requires certificates)
ENABLE_HTTPS=false
SSL_CERT_PATH=/certs/fullchain.pem
SSL_KEY_PATH=/certs/privkey.pem

# Rate limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS_PER_MINUTE=60

# =============================================================================
# Logging Configuration
# =============================================================================
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FORMAT=json  # json or text
LOG_DIR=/var/log/familyai

# =============================================================================
# K3s Configuration (for production deployment)
# =============================================================================
K3S_NAMESPACE=familyai
K3S_STORAGE_CLASS=local-path

# Ingress configuration
INGRESS_ENABLED=true
INGRESS_DOMAIN=familyai.local
INGRESS_TLS_ENABLED=false

# =============================================================================
# Development/Debug Options
# =============================================================================
# Enable debug mode
DEBUG=false

# Enable model preloading on startup
PRELOAD_MODELS=true

# Enable benchmarking mode
BENCHMARK_MODE=false

# Mock mode (use fake models for testing without GPU)
MOCK_MODE=false
